import os
import re
import glob
from bs4 import BeautifulSoup
import ebooklib
from ebooklib import epub

def clean_text(text):
    """
    Cleans the text by removing ads, system signals, and formatting it beautifully.
    """
    lines = text.splitlines()
    cleaned_lines = []
    
    # Heuristics for ads and system noise
    ad_keywords = [
        "Z-Library", "www.z-lib.org", "b-ok.cc", "b-ok.org", "bookzz.org", 
        "bookfi.net", "Uploaded by", "Generated by", "calibre"
    ]
    
    # Pass 1: Filter noise
    filtered_lines = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if any(keyword.lower() in line.lower() for keyword in ad_keywords):
            continue
        if line.isdigit() and len(line) < 4:
            continue
        filtered_lines.append(line)

    # Pass 2: Merge vertical text (consecutive single characters)
    merged_lines = []
    buffer = []
    
    for line in filtered_lines:
        # Check if line is a single character (Chinese or otherwise)
        # We allow a bit of looseness, e.g. single char + punctuation might be len 2 but visually similar
        # For safety, strictly 1 character or 1 char + punctuation? 
        # Let's stick to strictly len 1 for now to be safe, or just Chinese chars.
        # But commonly it's just one char per line in HTML.
        if len(line) == 1:
            buffer.append(line)
        else:
            if buffer:
                # Flush buffer
                merged_line = "".join(buffer)
                # If the merged line is very short (like < 2 chars), it might just be stray chars, but usually it's a title.
                # We add it as a paragraph.
                merged_lines.append("  " + merged_line)
                buffer = []
            
            # Add the current long line
            merged_lines.append("  " + line)
    
    # Flush remaining buffer
    if buffer:
        merged_lines.append("  " + "".join(buffer))
    
    cleaned_lines = merged_lines

    # Join with double newlines for clear paragraph separation
    return "\n\n".join(cleaned_lines)

def convert_epub_to_txt(epub_path, output_dir):
    try:
        book = epub.read_epub(epub_path)
    except Exception as e:
        print(f"Error reading {epub_path}: {e}")
        return

    base_name = os.path.basename(epub_path)
    txt_name = os.path.splitext(base_name)[0] + ".txt"
    output_path = os.path.join(output_dir, txt_name)
    
    print(f"Converting: {base_name} -> {output_path}")

    full_text = []

    # Iterate through items in the book
    # We prefer 'document' type items which usually contain the text
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            # Parse HTML content
            soup = BeautifulSoup(item.get_content(), 'html.parser')
            
            # Get text (get_text strips tags)
            # separator='\n' helps keep structure before we clean
            raw_text = soup.get_text(separator='\n')
            
            # Clean and format chunk
            cleaned_chunk = clean_text(raw_text)
            
            if cleaned_chunk:
                full_text.append(cleaned_chunk)

    # Final post-processing on the whole text
    final_content = "\n\n".join(full_text)
    
    # Remove excessive newlines (more than 3)
    final_content = re.sub(r'\n{4,}', '\n\n\n', final_content)

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_content)
    
    print(f"Finished: {txt_name}")

def main():
    input_dir = "input"
    output_dir = "output"
    
    if not os.path.exists(input_dir):
        print(f"Input directory '{input_dir}' not found.")
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    epub_files = glob.glob(os.path.join(input_dir, "*.epub"))
    
    if not epub_files:
        print(f"No epub files found in '{input_dir}'")
        return

    for epub_file in epub_files:
        convert_epub_to_txt(epub_file, output_dir)

if __name__ == "__main__":
    main()
